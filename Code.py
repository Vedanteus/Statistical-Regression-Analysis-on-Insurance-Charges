# -*- coding: utf-8 -*-
"""Code_Mohd Syukri_Tomer_Saeed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14KqyY9pIzctBN9NCpAVOMc7VOBTqJ5X2
"""

from pandas import  read_csv, DataFrame,Series,get_dummies
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn import linear_model
#from sklearn import linear_model
from sklearn.linear_model import SGDRegressor
from plotly import graph_objs, figure_factory

# reading and metadata
# Use csv file called "insurance_dataset_updated.csv" given in the zip file
insurance_df = read_csv("/content/insurance_dataset_updated.csv")
#print(data1.head(2))
print(insurance_df.shape)
print(insurance_df.info())
# # # # print(data1.describe())

# one hot encoding map
insurance_df['gender'] = insurance_df['gender'].map({'female': 1, 'male': 0})
insurance_df['smoker'] = insurance_df['smoker'].map({'yes': 1, 'no': 0})
insurance_df['medical_history'].fillna('No', inplace=True)
insurance_df['family_medical_history'].fillna('No', inplace=True)

#one hot encoding get dummies
insurance_final = get_dummies(insurance_df, columns =
 ['region',
  'exercise_frequency',
  'medical_history',
  'family_medical_history',
  'occupation',
  'coverage_level'],
drop_first=True) # encoding

insurance_final.info()

#Heatmap
correlation  = insurance_final.corr()
# print (correlation)
f_heatmap = figure_factory.create_annotated_heatmap(correlation.values,list(correlation.columns),list(correlation.columns),correlation.round(2).values,showscale=True)
f_heatmap.show()

X = insurance_final.drop(['charges'], axis = 1) # Features
Y = insurance_final['charges'] # Labels
X.info()
print(Y.shape)
print(X.shape)

X_scaled = StandardScaler().fit_transform(X)

# Linear Regressor No Penalty
LR = linear_model.SGDRegressor(random_state = 101, penalty = None) # building
LR_HP = {'eta0': [.001, .01, .1, 1], 'max_iter':[10000, 20000, 30000, 40000]}
GS_LR = GridSearchCV(estimator = LR, param_grid = LR_HP, scoring='r2', cv=10)
GS_LR.fit(X_scaled,Y)

# results = DataFrame.from_dict(GS_LR.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters_LR = GS_LR.best_params_
print("Best parameters: ", best_parameters_LR)
best_result_LR = GS_LR.best_score_
print("Best result: ", best_result_LR)
best_model_LR = GS_LR.best_estimator_
print("Intercept β0: ", best_model_LR.intercept_)
print(DataFrame(zip(X.columns, best_model_LR.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))

# Linear Regressor L2 (ridge)
LR2 = linear_model.SGDRegressor(random_state = 101, penalty = 'l2') # building
LR_HP2 = {'eta0': [.001, .01, .1, 1], 'max_iter':[10000, 20000, 30000, 40000], 'alpha': [.0001, .001, .01]}
GS_LR2 = GridSearchCV(estimator = LR2, param_grid = LR_HP2, scoring='r2', cv=10)
GS_LR2.fit(X_scaled,Y)

# results = DataFrame.from_dict(GS_LR2.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters_LR2 = GS_LR2.best_params_
print("Best parameters: ", best_parameters_LR2)
best_result_LR2 = GS_LR2.best_score_
print("Best result: ", best_result_LR2)
best_model_LR2 = GS_LR2.best_estimator_
print("Intercept β0: ", best_model_LR2.intercept_)
print(DataFrame(zip(X.columns, best_model_LR2.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=True))

# Linear Regressor L1 (lasso)
LR1 = linear_model.SGDRegressor(random_state = 101, penalty = 'l1') # building
LR_HP1 = {'eta0': [.001, .01, .1, 1], 'max_iter':[10000, 20000, 30000, 40000], 'alpha': [.0001, .001, .01]}
GS_LR1 = GridSearchCV(estimator = LR1, param_grid = LR_HP1, scoring='r2', cv=10)
GS_LR1.fit(X_scaled,Y)

# results = DataFrame.from_dict(GS_LR1.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters_LR1 = GS_LR1.best_params_
print("Best parameters: ", best_parameters_LR1)
best_result_LR1 = GS_LR1.best_score_
print("Best result: ", best_result_LR1)
best_model_LR1 = GS_LR1.best_estimator_
print("Intercept β0: ", best_model_LR1.intercept_)
print(DataFrame(zip(X.columns, best_model_LR1.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=True))

#  Lienar Regressor Elastic Net and Regularization
LR_EN = linear_model.SGDRegressor(random_state = 101, penalty = 'elasticnet')
HP_EN = {'eta0': [.0001, .001, .01], 'max_iter':[10000, 20000, 30000],'alpha': [.0001, .001, .01], 'l1_ratio': [.4, .5, .6]}
GS_EN = GridSearchCV(estimator=LR_EN, param_grid=HP_EN, scoring='r2', cv=5)
GS_EN.fit(X_scaled, Y)

# results = DataFrame.from_dict(grid_search2.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters_EN = GS_EN.best_params_
print("Best parameters: ", best_parameters_EN)
best_result_EN = GS_EN.best_score_
print("Best result: ", best_result_EN)
best_model_EN = GS_EN.best_estimator_
print("Intercept β0: ", best_model_EN.intercept_)
print(DataFrame(zip(X.columns, best_model_EN.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))

#Support Vector Regression without L2 Regularization
from sklearn.svm import SVR

SVRegressor = SVR()
# Tuning kernel and epsilon
SVR_HP = {'kernel': ['linear', 'rbf'], 'epsilon': [.001, .01, .1, 1]}
grid_search_SVR = GridSearchCV(SVR(), SVR_HP, cv=5, scoring='r2')
grid_search_SVR.fit(X_scaled, Y)

best_params_SVR = grid_search_SVR.best_params_
best_model_SVR = grid_search_SVR.best_estimator_

print("Best parameters: ", best_params_SVR)
best_result_SVR = grid_search_SVR.best_score_
print("Best result: ", best_result_SVR)
print("Intercept β0: ", best_model_SVR.intercept_)

# Displaying the coeficients
# Since the model is train using linear kernel we can get the coefficients
coefficients = best_model_SVR.coef_[0]  # Extracting the coefficients

# Creating a DataFrame with features and their coefficients
coefficients_df = DataFrame({'Features': X.columns, 'Coefficients': coefficients})

# Sorting the DataFrame by coefficients in descending order
coefficients_df = coefficients_df.sort_values(by='Coefficients', ascending=False)
print(coefficients_df)

#Support Vector Regression with L2 Regularization
from sklearn.svm import SVR

SVRegressor = SVR()
# Penalise the coefficient by adding Ridge or L2 regularization (C)
SVR_HP1 = {'kernel': ['linear', 'rbf'], 'C': [1, 100, 1000], 'epsilon': [.001, .01, .1, 1]}
grid_search_SVR1 = GridSearchCV(SVR(), SVR_HP1, cv=5, scoring='r2')
grid_search_SVR1.fit(X_scaled, Y)

best_params_SVR1 = grid_search_SVR1.best_params_
best_model_SVR1 = grid_search_SVR1.best_estimator_

print("Best parameters: ", best_params_SVR1)
best_result_SVR1 = grid_search_SVR1.best_score_
print("Best result: ", best_result_SVR1)
print("Intercept β0: ", best_model_SVR1.intercept_)

# Displaying the coeficients
# Since the model is train using linear kernel we can get the coefficients
coefficients = best_model_SVR1.coef_[0]  # Extracting the coefficients

# Creating a DataFrame with features and their coefficients
coefficients_df = DataFrame({'Features': X.columns, 'Coefficients': coefficients})

# Sorting the DataFrame by coefficients in descending order
coefficients_df = coefficients_df.sort_values(by='Coefficients', ascending=False)
print(coefficients_df)

# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
RF_Regressor = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [770, 780, 790]}
grid_search3 = GridSearchCV(estimator=RF_Regressor, param_grid=no_Trees, scoring='r2', cv=5)
grid_search3.fit(X_scaled, Y)

best_parameters = grid_search3.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search3.best_score_
print("best_score: ", best_result)
# modified_r2 = 1-(1-best_result)*(4/5*r-1)/(4/5t*r-c-1)
# print("modified_r2: ", modified_r2)
Important_feature = Series(grid_search3.best_estimator_.feature_importances_, index=list(X)).sort_values(ascending=False) # Getting feature importances list for the best model
print(Important_feature)

# Selecting features with higher sifnificance and redefining feature set
X_new = insurance_final[['smoker', 'coverage_level_Premium', 'medical_history_Heart disease', 'family_medical_history_Heart disease']]
X_scaled_new = StandardScaler().fit_transform(X_new)

RF_Regressor2 = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [500, 600, 700]}
grid_search4 = GridSearchCV(estimator=RF_Regressor2, param_grid=no_Trees, scoring='r2', cv=5)
grid_search4.fit(X_scaled_new, Y)

best_parameters = grid_search4.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search4.best_score_
print("r2: ", best_result)
#modified_r2 = 1-(1-best_result)*(4/5*r-1)/(4/5*r-c-1)
#print("modified_r2: ", modified_r2)
best_model = grid_search4.best_estimator_

import joblib
import csv

# Assuming X_scaled is your scaled feature matrix and Y is your target variable
# Save the model to a file
joblib.dump(best_model_EN, "SVR.pkl")

# Load the model from file
svr_model = joblib.load("SVR.pkl")

# Fit the model (if needed)
svr_model.fit(X_scaled, Y)  # Replace 'Y' with your target variable

# Assuming 'new_data.csv' contains your new data
# Use csv file called "newdata.csv" given in the zip file
new_data = read_csv("/content/newdata.csv")
# one hot encoding map
new_data['gender'] = new_data['gender'].map({'female': 1, 'male': 0})
new_data['smoker'] = new_data['smoker'].map({'yes': 1, 'no': 0})
new_data['medical_history'].fillna('No', inplace=True)
new_data['family_medical_history'].fillna('No', inplace=True)

#one hot encoding get dummies
new_data = get_dummies(new_data, columns =
 ['region',
  'exercise_frequency',
  'medical_history',
  'family_medical_history',
  'occupation',
  'coverage_level'],
drop_first=True) # encoding

new_data_final = new_data.drop(['charges'], axis = 1) # Features
new_data_scaled = StandardScaler().fit_transform(new_data_final)
# Make predictions
predictions = svr_model.predict(new_data_scaled)
formatted_predictions = ["{:.2f}".format(pred) for pred in predictions]

# Display the formatted predictions
print(formatted_predictions)

# Define the file path where you want to create the CSV file
file_path = 'predictions_charges.csv'

# Write the predictions to a CSV file
with open(file_path, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Predicted Charges'])  # Write header if needed
    writer.writerows(map(lambda x: [x], formatted_predictions))

prediction_charges = read_csv('/content/predictions_charges.csv')

# Assuming 'column_to_add' is the specific column from new_data.csv to add
prediction_charges['charges'] = new_data['charges']

prediction_charges.head(10)